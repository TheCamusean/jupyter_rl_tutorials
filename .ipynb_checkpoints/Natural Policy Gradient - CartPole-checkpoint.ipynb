{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient():\n",
    "    with tf.variable_scope(\"policy\"):\n",
    "        params = tf.get_variable(\"policy_parameters\", [4,2])\n",
    "        state = tf.placeholder(\"float\", [None, 4], name=\"state\")\n",
    "        # NOTE: have to specify shape of actions so we can call\n",
    "        # get_shape when calculating g_log_prob below\n",
    "        actions = tf.placeholder(\"float\", [200, 2], name=\"actions\")\n",
    "        advantages = tf.placeholder(\"float\", [None,], name=\"advantages\")\n",
    "        linear = tf.matmul(state, params)\n",
    "        probabilities = tf.nn.softmax(linear)\n",
    "        my_variables = tf.trainable_variables()\n",
    "\n",
    "        # calculate the probability of the chosen action given the state\n",
    "        action_log_prob = tf.log(tf.reduce_sum(\n",
    "            tf.multiply(probabilities, actions), reduction_indices=[1]))\n",
    "\n",
    "        # calculate the gradient of the log probability at each point in time\n",
    "        # NOTE: doing this because tf.gradients only returns a summed version\n",
    "        action_log_prob_flat = tf.reshape(action_log_prob, (-1,))\n",
    "\n",
    "        g_log_prob = tf.stack(\n",
    "            [tf.gradients(action_log_prob_flat[i], my_variables)[0]\n",
    "                for i in range(action_log_prob_flat.get_shape()[0])])\n",
    "        g_log_prob = tf.reshape(g_log_prob, (200, 8, 1))\n",
    "\n",
    "        # calculate the policy gradient by multiplying by the advantage function\n",
    "        g = tf.multiply(g_log_prob, tf.reshape(advantages, (200, 1, 1)))\n",
    "        # sum over time\n",
    "        g = 1.00 / 200.00 * tf.reduce_sum(g, reduction_indices=[0])\n",
    "\n",
    "        # calculate the Fischer information matrix and its inverse\n",
    "        F2 = tf.map_fn(lambda x: tf.matmul(x, tf.transpose(x)), g_log_prob)\n",
    "        F = 1.0 / 200.0 * tf.reduce_sum(F2, reduction_indices=[0])\n",
    "\n",
    "        # calculate inverse of positive definite clipped F\n",
    "        # NOTE: have noticed small eigenvalues (1e-10) that are negative,\n",
    "        # using SVD to clip those out, assuming they're rounding errors\n",
    "        S, U, V = tf.svd(F)\n",
    "        atol = tf.reduce_max(S) * 1e-6\n",
    "        S_inv = tf.divide(1.0, S)\n",
    "        S_inv = tf.where(S < atol, tf.zeros_like(S), S_inv)\n",
    "        S_inv = tf.diag(S_inv)\n",
    "        F_inv = tf.matmul(S_inv, tf.transpose(U))\n",
    "        F_inv = tf.matmul(V, F_inv)\n",
    "\n",
    "        # calculate natural policy gradient ascent update\n",
    "        F_inv_g = tf.matmul(F_inv, g)\n",
    "        # calculate a learning rate normalized such that a constant change\n",
    "        # in the output control policy is achieved each update, preventing\n",
    "        # any parameter changes that hugely change the output\n",
    "        learning_rate = tf.sqrt(\n",
    "            tf.divide(0.001, tf.matmul(tf.transpose(g), F_inv_g)))\n",
    "\n",
    "        update = tf.multiply(learning_rate, F_inv_g)\n",
    "        update = tf.reshape(update, (4, 2))\n",
    "\n",
    "        # update trainable parameters\n",
    "        # NOTE: whenever my_variables is fetched they're also updated\n",
    "        my_variables[0] = tf.assign_add(my_variables[0], update)\n",
    "\n",
    "        return probabilities, state, actions, advantages, my_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_gradient():\n",
    "    with tf.variable_scope(\"value\"):\n",
    "        state = tf.placeholder(\"float\", [None, 4])\n",
    "        newvals = tf.placeholder(\"float\", [None, 1])\n",
    "        w1 = tf.get_variable(\"w1\", [4, 10])\n",
    "        b1 = tf.get_variable(\"b1\", [10])\n",
    "        h1 = tf.nn.relu(tf.matmul(state,w1) + b1)\n",
    "        w2 = tf.get_variable(\"w2\", [10, 1])\n",
    "        b2 = tf.get_variable(\"b2\", [1])\n",
    "        calculated = tf.matmul(h1, w2) + b2\n",
    "\n",
    "        # minimize the difference between predicted and actual\n",
    "        diffs = calculated - newvals\n",
    "        loss = tf.nn.l2_loss(diffs)\n",
    "        optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "        return calculated, state, newvals, optimizer, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy_grad, value_grad, sess):\n",
    "    # unpack the policy network (generates control policy)\n",
    "    (pl_calculated, pl_state, pl_actions,\n",
    "        pl_advantages, pl_optimizer) = policy_grad\n",
    "    # unpack the value network (estimates expected reward)\n",
    "    (vl_calculated, vl_state, vl_newvals,\n",
    "        vl_optimizer, vl_loss) = value_grad\n",
    "\n",
    "    # set up the environment\n",
    "    observation = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    total_rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    advantages = []\n",
    "    transitions = []\n",
    "    update_vals = []\n",
    "\n",
    "    n_episodes = 0\n",
    "    n_timesteps = 200\n",
    "    for t in range(n_timesteps):\n",
    "        # calculate policy\n",
    "        obs_vector = np.expand_dims(observation, axis=0)\n",
    "        probs = sess.run(\n",
    "            pl_calculated,\n",
    "            feed_dict={pl_state: obs_vector})\n",
    "\n",
    "        # stochastically generate action using the policy output\n",
    "        action = 0 if random.uniform(0,1) < probs[0][0] else 1\n",
    "        # record the transition\n",
    "        states.append(observation)\n",
    "        actionblank = np.zeros(2)\n",
    "        actionblank[action] = 1\n",
    "        actions.append(actionblank)\n",
    "        # take the action in the environment\n",
    "        old_observation = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        transitions.append((old_observation, action, reward))\n",
    "        episode_reward += reward\n",
    "\n",
    "        # if the pole falls or time is up\n",
    "        if done or t == n_timesteps - 1:\n",
    "            for ii, trans in enumerate(transitions):\n",
    "                obs, action, reward = trans\n",
    "\n",
    "                # calculate discounted monte-carlo return\n",
    "                future_reward = 0\n",
    "                future_transitions = len(transitions) - ii\n",
    "                decrease = 1\n",
    "                for jj in range(future_transitions):\n",
    "                    future_reward += transitions[jj + ii][2] * decrease\n",
    "                    decrease = decrease * 0.97\n",
    "                obs_vector = np.expand_dims(obs, axis=0)\n",
    "                # compare the calculated expected reward to the average\n",
    "                # expected reward, as estimated by the value network\n",
    "                currentval = sess.run(\n",
    "                    vl_calculated, feed_dict={vl_state: obs_vector})[0][0]\n",
    "\n",
    "                # advantage: how much better was this action than normal\n",
    "                advantages.append(future_reward - currentval)\n",
    "\n",
    "                # update the value function towards new return\n",
    "                update_vals.append(future_reward)\n",
    "\n",
    "            n_episodes += 1\n",
    "            # reset variables for next episode in batch\n",
    "            total_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            transitions = []\n",
    "\n",
    "            if done:\n",
    "                # if the pole fell, reset environment\n",
    "                observation = env.reset()\n",
    "            else:\n",
    "                # if out of time, close environment\n",
    "                env.close()\n",
    "\n",
    "    print('total_rewards: ', total_rewards)\n",
    "\n",
    "    # update value function\n",
    "    update_vals_vector = np.expand_dims(update_vals, axis=1)\n",
    "    sess.run(vl_optimizer,\n",
    "             feed_dict={vl_state: states,\n",
    "                        vl_newvals: update_vals_vector})\n",
    "    # update control policy\n",
    "    sess.run(pl_optimizer,\n",
    "             feed_dict={pl_state: states,\n",
    "                        pl_advantages: advantages,\n",
    "                        pl_actions: actions})\n",
    "\n",
    "    return total_rewards, n_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "total_rewards:  [69.0, 70.0, 39.0, 22.0]\n",
      "total_rewards:  [41.0, 48.0, 87.0, 24.0]\n",
      "total_rewards:  [60.0, 59.0, 23.0, 33.0, 25.0]\n",
      "total_rewards:  [31.0, 16.0, 105.0, 48.0]\n",
      "total_rewards:  [60.0, 76.0, 33.0, 31.0]\n",
      "total_rewards:  [43.0, 67.0, 44.0, 46.0]\n",
      "total_rewards:  [29.0, 79.0, 86.0, 6.0]\n",
      "total_rewards:  [55.0, 44.0, 37.0, 64.0]\n",
      "total_rewards:  [41.0, 51.0, 61.0, 47.0]\n",
      "total_rewards:  [74.0, 53.0, 47.0, 26.0]\n",
      "total_rewards:  [69.0, 29.0, 102.0]\n",
      "total_rewards:  [49.0, 61.0, 60.0, 30.0]\n",
      "total_rewards:  [108.0, 84.0, 8.0]\n",
      "total_rewards:  [76.0, 68.0, 50.0, 6.0]\n",
      "total_rewards:  [71.0, 49.0, 54.0, 26.0]\n",
      "total_rewards:  [56.0, 69.0, 50.0, 25.0]\n",
      "total_rewards:  [69.0, 36.0, 28.0, 48.0, 19.0]\n",
      "total_rewards:  [92.0, 56.0, 33.0, 19.0]\n",
      "total_rewards:  [65.0, 33.0, 57.0, 45.0]\n",
      "total_rewards:  [79.0, 41.0, 63.0, 17.0]\n",
      "total_rewards:  [61.0, 49.0, 47.0, 43.0]\n",
      "total_rewards:  [61.0, 72.0, 57.0, 10.0]\n",
      "total_rewards:  [143.0, 43.0, 14.0]\n",
      "total_rewards:  [38.0, 95.0, 46.0, 21.0]\n",
      "total_rewards:  [65.0, 50.0, 44.0, 41.0]\n",
      "total_rewards:  [42.0, 111.0, 47.0]\n",
      "total_rewards:  [73.0, 32.0, 79.0, 16.0]\n",
      "total_rewards:  [76.0, 78.0, 46.0]\n",
      "total_rewards:  [79.0, 85.0, 36.0]\n",
      "total_rewards:  [37.0, 63.0, 99.0, 1.0]\n",
      "total_rewards:  [58.0, 69.0, 64.0, 9.0]\n",
      "total_rewards:  [48.0, 92.0, 60.0]\n",
      "total_rewards:  [42.0, 71.0, 87.0]\n",
      "total_rewards:  [131.0, 69.0]\n",
      "total_rewards:  [51.0, 57.0, 58.0, 34.0]\n",
      "total_rewards:  [120.0, 64.0, 16.0]\n",
      "total_rewards:  [105.0, 68.0, 27.0]\n",
      "total_rewards:  [81.0, 73.0, 46.0]\n",
      "total_rewards:  [63.0, 137.0]\n",
      "total_rewards:  [68.0, 124.0, 8.0]\n",
      "total_rewards:  [48.0, 135.0, 17.0]\n",
      "total_rewards:  [129.0, 71.0]\n",
      "total_rewards:  [89.0, 77.0, 34.0]\n",
      "total_rewards:  [68.0, 92.0, 40.0]\n",
      "total_rewards:  [69.0, 54.0, 66.0, 11.0]\n",
      "total_rewards:  [117.0, 83.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [62.0, 90.0, 48.0]\n",
      "total_rewards:  [94.0, 106.0]\n",
      "total_rewards:  [122.0, 78.0]\n",
      "total_rewards:  [113.0, 87.0]\n",
      "total_rewards:  [147.0, 53.0]\n",
      "total_rewards:  [107.0, 86.0, 7.0]\n",
      "total_rewards:  [109.0, 91.0]\n",
      "total_rewards:  [106.0, 85.0, 9.0]\n",
      "total_rewards:  [80.0, 120.0]\n",
      "total_rewards:  [164.0, 36.0]\n",
      "total_rewards:  [109.0, 91.0]\n",
      "total_rewards:  [151.0, 49.0]\n",
      "total_rewards:  [129.0, 71.0]\n",
      "total_rewards:  [122.0, 65.0, 13.0]\n",
      "total_rewards:  [67.0, 133.0]\n",
      "total_rewards:  [139.0, 61.0]\n",
      "total_rewards:  [139.0, 61.0]\n",
      "total_rewards:  [180.0, 20.0]\n",
      "total_rewards:  [73.0, 127.0]\n",
      "total_rewards:  [134.0, 66.0]\n",
      "total_rewards:  [156.0, 44.0]\n",
      "total_rewards:  [179.0, 21.0]\n",
      "total_rewards:  [131.0, 69.0]\n",
      "total_rewards:  [105.0, 95.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [164.0, 36.0]\n",
      "total_rewards:  [139.0, 61.0]\n",
      "total_rewards:  [179.0, 21.0]\n",
      "total_rewards:  [175.0, 25.0]\n",
      "total_rewards:  [127.0, 73.0]\n",
      "total_rewards:  [163.0, 37.0]\n",
      "total_rewards:  [131.0, 69.0]\n",
      "total_rewards:  [132.0, 68.0]\n",
      "total_rewards:  [118.0, 82.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [130.0, 70.0]\n",
      "total_rewards:  [124.0, 76.0]\n",
      "total_rewards:  [179.0, 21.0]\n",
      "total_rewards:  [146.0, 54.0]\n",
      "total_rewards:  [137.0, 63.0]\n",
      "total_rewards:  [138.0, 62.0]\n",
      "total_rewards:  [166.0, 34.0]\n",
      "total_rewards:  [130.0, 70.0]\n",
      "total_rewards:  [173.0, 27.0]\n",
      "total_rewards:  [92.0, 108.0]\n",
      "total_rewards:  [94.0, 106.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [171.0, 29.0]\n",
      "total_rewards:  [140.0, 60.0]\n",
      "total_rewards:  [149.0, 51.0]\n",
      "total_rewards:  [174.0, 26.0]\n",
      "total_rewards:  [131.0, 69.0]\n",
      "100\n",
      "total_rewards:  [193.0, 7.0]\n",
      "total_rewards:  [159.0, 41.0]\n",
      "total_rewards:  [149.0, 51.0]\n",
      "total_rewards:  [187.0, 13.0]\n",
      "total_rewards:  [178.0, 22.0]\n",
      "total_rewards:  [148.0, 52.0]\n",
      "total_rewards:  [153.0, 47.0]\n",
      "total_rewards:  [152.0, 48.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [176.0, 24.0]\n",
      "total_rewards:  [185.0, 15.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [180.0, 20.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [162.0, 38.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [159.0, 41.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [192.0, 8.0]\n",
      "total_rewards:  [191.0, 9.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [161.0, 39.0]\n",
      "total_rewards:  [173.0, 27.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [76.0, 124.0]\n",
      "total_rewards:  [105.0, 95.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [178.0, 22.0]\n",
      "total_rewards:  [192.0, 8.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [173.0, 27.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [174.0, 26.0]\n",
      "total_rewards:  [181.0, 19.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [157.0, 43.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [189.0, 11.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [188.0, 12.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [182.0, 18.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [186.0, 14.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [175.0, 25.0]\n",
      "total_rewards:  [171.0, 29.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [187.0, 13.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [177.0, 23.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [170.0, 30.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [167.0, 33.0]\n",
      "200\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [195.0, 5.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [174.0, 26.0]\n",
      "total_rewards:  [114.0, 86.0]\n",
      "total_rewards:  [114.0, 86.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [118.0, 81.0, 1.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [132.0, 68.0]\n",
      "total_rewards:  [148.0, 52.0]\n",
      "total_rewards:  [86.0, 80.0, 34.0]\n",
      "total_rewards:  [117.0, 83.0]\n",
      "total_rewards:  [88.0, 104.0, 8.0]\n",
      "total_rewards:  [127.0, 73.0]\n",
      "total_rewards:  [135.0, 65.0]\n",
      "total_rewards:  [195.0, 5.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [137.0, 63.0]\n",
      "total_rewards:  [176.0, 24.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [177.0, 23.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [192.0, 8.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [170.0, 30.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [135.0, 55.0, 10.0]\n",
      "total_rewards:  [94.0, 68.0, 38.0]\n",
      "total_rewards:  [131.0, 69.0]\n",
      "total_rewards:  [165.0, 35.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [158.0, 42.0]\n",
      "total_rewards:  [62.0, 138.0]\n",
      "total_rewards:  [153.0, 47.0]\n",
      "total_rewards:  [185.0, 15.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [184.0, 16.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [150.0, 50.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [135.0, 65.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [184.0, 16.0]\n",
      "total_rewards:  [158.0, 42.0]\n",
      "total_rewards:  [177.0, 23.0]\n",
      "total_rewards:  [168.0, 32.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [181.0, 19.0]\n",
      "total_rewards:  [192.0, 8.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [188.0, 12.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [145.0, 55.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [155.0, 45.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [188.0, 12.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_rewards:  [178.0, 22.0]\n",
      "total_rewards:  [163.0, 37.0]\n",
      "total_rewards:  [114.0, 86.0]\n",
      "total_rewards:  [145.0, 47.0, 8.0]\n",
      "total_rewards:  [149.0, 51.0]\n",
      "total_rewards:  [158.0, 42.0]\n",
      "total_rewards:  [160.0, 40.0]\n",
      "total_rewards:  [134.0, 66.0]\n",
      "total_rewards:  [174.0, 26.0]\n",
      "total_rewards:  [135.0, 65.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [106.0, 94.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [200.0]\n",
      "average time: 0.090\n",
      "0\n",
      "total_rewards:  [17.0, 19.0, 26.0, 12.0, 38.0, 17.0, 13.0, 15.0, 26.0, 11.0, 6.0]\n",
      "total_rewards:  [14.0, 11.0, 10.0, 10.0, 20.0, 27.0, 18.0, 14.0, 22.0, 12.0, 31.0, 11.0]\n",
      "total_rewards:  [20.0, 22.0, 16.0, 12.0, 8.0, 12.0, 11.0, 25.0, 9.0, 39.0, 24.0, 2.0]\n",
      "total_rewards:  [16.0, 20.0, 31.0, 14.0, 24.0, 17.0, 18.0, 12.0, 16.0, 15.0, 17.0]\n",
      "total_rewards:  [36.0, 18.0, 28.0, 23.0, 16.0, 12.0, 18.0, 24.0, 25.0]\n",
      "total_rewards:  [11.0, 19.0, 14.0, 15.0, 23.0, 20.0, 23.0, 23.0, 14.0, 21.0, 17.0]\n",
      "total_rewards:  [17.0, 14.0, 41.0, 22.0, 17.0, 12.0, 30.0, 33.0, 14.0]\n",
      "total_rewards:  [12.0, 69.0, 25.0, 27.0, 21.0, 18.0, 10.0, 18.0]\n",
      "total_rewards:  [14.0, 19.0, 14.0, 19.0, 11.0, 28.0, 13.0, 19.0, 19.0, 17.0, 21.0, 6.0]\n",
      "total_rewards:  [61.0, 24.0, 49.0, 12.0, 28.0, 16.0, 10.0]\n",
      "total_rewards:  [12.0, 15.0, 35.0, 18.0, 14.0, 23.0, 18.0, 30.0, 31.0, 4.0]\n",
      "total_rewards:  [21.0, 30.0, 11.0, 25.0, 19.0, 15.0, 10.0, 15.0, 30.0, 21.0, 3.0]\n",
      "total_rewards:  [10.0, 22.0, 13.0, 35.0, 47.0, 13.0, 21.0, 15.0, 24.0]\n",
      "total_rewards:  [12.0, 14.0, 16.0, 11.0, 21.0, 36.0, 23.0, 41.0, 22.0, 4.0]\n",
      "total_rewards:  [39.0, 22.0, 15.0, 16.0, 70.0, 24.0, 14.0]\n",
      "total_rewards:  [19.0, 34.0, 29.0, 34.0, 21.0, 24.0, 39.0]\n",
      "total_rewards:  [23.0, 43.0, 15.0, 19.0, 13.0, 23.0, 38.0, 22.0, 4.0]\n",
      "total_rewards:  [72.0, 28.0, 60.0, 40.0]\n",
      "total_rewards:  [56.0, 28.0, 55.0, 19.0, 30.0, 12.0]\n",
      "total_rewards:  [29.0, 16.0, 21.0, 27.0, 36.0, 23.0, 16.0, 20.0, 12.0]\n",
      "total_rewards:  [18.0, 21.0, 23.0, 20.0, 25.0, 44.0, 49.0]\n",
      "total_rewards:  [24.0, 85.0, 65.0, 17.0, 9.0]\n",
      "total_rewards:  [17.0, 36.0, 13.0, 62.0, 25.0, 33.0, 11.0, 3.0]\n",
      "total_rewards:  [59.0, 12.0, 44.0, 42.0, 27.0, 16.0]\n",
      "total_rewards:  [20.0, 68.0, 109.0, 3.0]\n",
      "total_rewards:  [70.0, 49.0, 81.0]\n",
      "total_rewards:  [73.0, 18.0, 109.0]\n",
      "total_rewards:  [106.0, 94.0]\n",
      "total_rewards:  [93.0, 29.0, 28.0, 32.0, 18.0]\n",
      "total_rewards:  [99.0, 35.0, 66.0]\n",
      "total_rewards:  [95.0, 105.0]\n",
      "total_rewards:  [158.0, 42.0]\n",
      "total_rewards:  [97.0, 103.0]\n",
      "total_rewards:  [94.0, 19.0, 70.0, 17.0]\n",
      "total_rewards:  [200.0]\n",
      "total_rewards:  [15.0, 185.0]\n",
      "total_rewards:  [163.0, 37.0]\n"
     ]
    }
   ],
   "source": [
    "# generate the networks\n",
    "policy_grad = policy_gradient()\n",
    "value_grad = value_gradient()\n",
    "\n",
    "# run the training from scratch 10 times, record results\n",
    "for ii in range(10):\n",
    "\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env = gym.wrappers.Monitor(\n",
    "        env=env,\n",
    "        directory='cartpole-hill/',\n",
    "        force=True,\n",
    "        video_callable=False)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if not os.path.exists('natural'):\n",
    "        os.mkdir('natural')\n",
    "    if not os.path.exists(os.path.join('natural','third')):\n",
    "        os.mkdir(os.path.join('natural','third'))\n",
    "\n",
    "    summ_writer_3 = tf.summary.FileWriter(os.path.join('natural','third'), sess.graph)\n",
    "\n",
    "\n",
    "    max_rewards = []\n",
    "    total_episodes = []\n",
    "    # each batch is 200 time steps worth of episodes\n",
    "    n_training_batches = 300\n",
    "    import time\n",
    "    times = []\n",
    "    for i in range(n_training_batches):\n",
    "        start_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        reward, n_episodes = run_episode(env, policy_grad, value_grad, sess)\n",
    "        max_rewards.append(np.max(reward))\n",
    "        total_episodes.append(n_episodes)\n",
    "        times.append(time.time() - start_time)\n",
    "    print('average time: %.3f' % (np.sum(times) / n_training_batches))\n",
    "\n",
    "    #np.savez_compressed('data/natural_policy_gradient_%i' % ii,\n",
    "    #        max_rewards=max_rewards, total_episodes=total_episodes)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "obs_vector = np.expand_dims(observation, axis=0)\n",
    "obs_vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5079033 , 0.49209666]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pl_calculated, pl_state, pl_actions,\n",
    "    pl_advantages, pl_optimizer) = policy_grad\n",
    "\n",
    "pl_calculated\n",
    "   \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "probs = sess.run(\n",
    "    pl_calculated,\n",
    "    feed_dict={pl_state: obs_vector})\n",
    "\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Graph' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8c4260df1138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpl_calculated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Graph' object is not callable"
     ]
    }
   ],
   "source": [
    "pl_calculated.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03047183,  0.00344319, -0.00064845,  0.01113388]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
